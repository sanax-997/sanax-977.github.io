<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Curriculum Vitae Sandro Schweiss]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>Curriculum Vitae Sandro Schweiss</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 08 May 2025 07:03:15 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 08 May 2025 07:03:04 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[Home Page]]></title><description><![CDATA[ 
 <br>Curriculum Vitae of Ing. Sandro Schweiss, B.Sc.<br>4 years of overall experience in Project Management<br>
<br>2 years of experience in Data Science
<br>2 years of experience in technical 2D/3D Design
<br><br>04.2020 – 12.2023: <a data-href="Marco Treppen" href="00-index\marco-treppen.html" class="internal-link" target="_self" rel="noopener nofollow">Marco Treppen</a> - Mechatronics Engineer<br>
12.2019 – 02.2020: <a data-href="Lehner Electronics" href="00-index\lehner-electronics.html" class="internal-link" target="_self" rel="noopener nofollow">Lehner Electronics</a> - Project Manager <br><br>05.2020 - 11.2024: <a data-href="International University (IU)" href="00-index\international-university-(iu).html" class="internal-link" target="_self" rel="noopener nofollow">International University (IU)</a> - B.Sc. Data Science<br>
09.2014 - 05.2019: <a data-href="HTL Eisenstadt" href="00-index\htl-eisenstadt.html" class="internal-link" target="_self" rel="noopener nofollow">HTL Eisenstadt</a> - Mechatronics Engineer - Matura<br><br>12.2023: <a data-href="Professional Title - Ingenieur (Ing.)" href="01-entries\1.3-certifications-or-professional-development\professional-title-ingenieur-(ing.).html" class="internal-link" target="_self" rel="noopener nofollow">Professional Title - Ingenieur (Ing.)</a> - Austrian Federal Ministry of Education ]]></description><link>00-index\home-page.html</link><guid isPermaLink="false">00 - Index/Home Page.md</guid><pubDate>Thu, 08 May 2025 07:02:37 GMT</pubDate></item><item><title><![CDATA[HTL Eisenstadt]]></title><description><![CDATA[ 
 <br>From September 2014 to May 2019 I completed a five-year technical education at HTL Eisenstadt with a specialization in Mechatronics Engineering. The curriculum provided a comprehensive foundation across mechanical design, electronics, control systems, automation technology, and computer science. Over the course of my studies, I developed strong practical and theoretical expertise, particularly in technical 2D and 3D modeling and design. These skills became a central focus of my education and ultimately culminated in my final thesis, where I served as team lead and I applied them for the design of a real-world project. I successfully graduated with the "Matura" (Austrian high school diploma with university entrance qualification), and after gaining three years of industry experience, I was awarded the professional title Ingenieur (Ing.), formally recognizing both my academic and applied technical competence.<br>
<br>Final Thesis - <a data-href="Modular Trade Fair Stand Design &amp; Interactive Exhibition Toolkit" href="01-entries\1.2-education\htl-eisenstadt-entries\modular-trade-fair-stand-design-&amp;-interactive-exhibition-toolkit.html" class="internal-link" target="_self" rel="noopener nofollow">Modular Trade Fair Stand Design &amp; Interactive Exhibition Toolkit</a>
<br>Technical report - <a data-href="Development of the RE-PORT 5000 Automated Quality Control System" href="01-entries\1.2-education\htl-eisenstadt-entries\development-of-the-re-port-5000-automated-quality-control-system.html" class="internal-link" target="_self" rel="noopener nofollow">Development of the RE-PORT 5000 Automated Quality Control System</a>
<br>Technical report - <a data-href="Development of the KAR-MA 4000 Vertical Carousel Storage System" href="01-entries\1.2-education\htl-eisenstadt-entries\development-of-the-kar-ma-4000-vertical-carousel-storage-system.html" class="internal-link" target="_self" rel="noopener nofollow">Development of the KAR-MA 4000 Vertical Carousel Storage System</a>
<br>Paper - <a data-href="Trennung der Variablen &amp; Variation der Konstanten" href="01-entries\1.2-education\htl-eisenstadt-entries\trennung-der-variablen-&amp;-variation-der-konstanten.html" class="internal-link" target="_self" rel="noopener nofollow">Trennung der Variablen &amp; Variation der Konstanten</a>
]]></description><link>00-index\htl-eisenstadt.html</link><guid isPermaLink="false">00 - Index/HTL Eisenstadt.md</guid><pubDate>Fri, 02 May 2025 07:45:09 GMT</pubDate></item><item><title><![CDATA[International University (IU)]]></title><description><![CDATA[ 
 <br>From May 2020 to November 2024, I completed my Bachelor's degree in Data Science at the International University (IU). Throughout my studies, I worked on a wide range of projects covering core areas such as Python programming, SQL database design, mathematics and statistics, business intelligence, software development principles, machine learning, deep learning, and data-related security. I also gained hands-on experience with DevOps practices, cloud computing, big data technologies, and agile project management. My academic journey was strongly application-focused, culminating in a Bachelor thesis on large language models (LLMs). <br><br>The following is an overview of my most notable and relevant projects.<br>
<br><a data-href="Airbnb Data Mart Development – SQL &amp; Python Integration" href="01-entries\1.2-education\iu-entries\airbnb-data-mart-development-–-sql-&amp;-python-integration.html" class="internal-link" target="_self" rel="noopener nofollow">Airbnb Data Mart Development – SQL &amp; Python Integration</a>
<br><a data-href="Habit Tracker – A Web-Based Habit Management &amp; Analytics Tool in Python" href="01-entries\1.2-education\iu-entries\habit-tracker-–-a-web-based-habit-management-&amp;-analytics-tool-in-python.html" class="internal-link" target="_self" rel="noopener nofollow">Habit Tracker – A Web-Based Habit Management &amp; Analytics Tool in Python</a>
<br><a data-href="Mental Health in Technology-Related Jobs – An Exploratory Data Science Case Study" href="01-entries\1.2-education\iu-entries\mental-health-in-technology-related-jobs-–-an-exploratory-data-science-case-study.html" class="internal-link" target="_self" rel="noopener nofollow">Mental Health in Technology-Related Jobs – An Exploratory Data Science Case Study</a>
<br><a data-href="Image Classification for a Refund Department – End-to-End ML Deployment in Python" href="01-entries\1.2-education\iu-entries\image-classification-for-a-refund-department-–-end-to-end-ml-deployment-in-python.html" class="internal-link" target="_self" rel="noopener nofollow">Image Classification for a Refund Department – End-to-End ML Deployment in Python</a>
<br><a data-href="Geo-Spatial Energy Analysis – Interactive Data Visualization of Global Energy Trends" href="01-entries\1.2-education\iu-entries\geo-spatial-energy-analysis-–-interactive-data-visualization-of-global-energy-trends.html" class="internal-link" target="_self" rel="noopener nofollow">Geo-Spatial Energy Analysis – Interactive Data Visualization of Global Energy Trends</a>
<br><a data-href="Demand Forecasting Model for Public Transport – Predictive Time Series Analysis in Python" href="01-entries\1.2-education\iu-entries\demand-forecasting-model-for-public-transport-–-predictive-time-series-analysis-in-python.html" class="internal-link" target="_self" rel="noopener nofollow">Demand Forecasting Model for Public Transport – Predictive Time Series Analysis in Python</a>
<br><a data-href="Sentiment Analysis of Customer Reviews – NLP Pipeline Using Pre-Trained BERT Models" href="01-entries\1.2-education\iu-entries\sentiment-analysis-of-customer-reviews-–-nlp-pipeline-using-pre-trained-bert-models.html" class="internal-link" target="_self" rel="noopener nofollow">Sentiment Analysis of Customer Reviews – NLP Pipeline Using Pre-Trained BERT Models</a>
<br><a data-href="NLP Topic Modeling of Customer Complaints – A Comparative Approach Using LDA and BERTopic" href="01-entries\1.2-education\iu-entries\nlp-topic-modeling-of-customer-complaints-–-a-comparative-approach-using-lda-and-bertopic.html" class="internal-link" target="_self" rel="noopener nofollow">NLP Topic Modeling of Customer Complaints – A Comparative Approach Using LDA and BERTopic</a>
<br><a data-href="Performance improvement of LLMs for NER using Ensemble Learning techniques" href="01-entries\1.2-education\iu-entries\performance-improvement-of-llms-for-ner-using-ensemble-learning-techniques.html" class="internal-link" target="_self" rel="noopener nofollow">Performance improvement of LLMs for NER using Ensemble Learning techniques</a>
<br><br>
<br><a data-href="Ethical Prinicples of a Global Company" href="01-entries\1.2-education\iu-entries\ethical-prinicples-of-a-global-company.html" class="internal-link" target="_self" rel="noopener nofollow">Ethical Prinicples of a Global Company</a>
<br><a data-href="Testing statistical hypotheses using non-parametric tests" href="01-entries\1.2-education\iu-entries\testing-statistical-hypotheses-using-non-parametric-tests.html" class="internal-link" target="_self" rel="noopener nofollow">Testing statistical hypotheses using non-parametric tests</a>
<br><a data-href="Architecture Proposal for Constructing a Data Warehouse" href="01-entries\1.2-education\iu-entries\architecture-proposal-for-constructing-a-data-warehouse.html" class="internal-link" target="_self" rel="noopener nofollow">Architecture Proposal for Constructing a Data Warehouse</a>
<br><a data-href="Principles of Agility used in Modern Software Development" href="01-entries\1.2-education\iu-entries\principles-of-agility-used-in-modern-software-development.html" class="internal-link" target="_self" rel="noopener nofollow">Principles of Agility used in Modern Software Development</a>
<br><a data-href="Introduction to Academic Work" href="01-entries\1.2-education\iu-entries\introduction-to-academic-work.html" class="internal-link" target="_self" rel="noopener nofollow">Introduction to Academic Work</a>
<br><a data-href="Enhancement of an Online Retailer's Data Quality Management Capabilities" href="01-entries\1.2-education\iu-entries\enhancement-of-an-online-retailer's-data-quality-management-capabilities.html" class="internal-link" target="_self" rel="noopener nofollow">Enhancement of an Online Retailer's Data Quality Management Capabilities</a>
<br><a data-href="How to Deal with Security and Privacy Threats in the Context of Data Science" href="01-entries\1.2-education\iu-entries\how-to-deal-with-security-and-privacy-threats-in-the-context-of-data-science.html" class="internal-link" target="_self" rel="noopener nofollow">How to Deal with Security and Privacy Threats in the Context of Data Science</a>
<br><a data-href="What is the Historical Development of CAR-2-X Technology" href="01-entries\1.2-education\iu-entries\what-is-the-historical-development-of-car-2-x-technology.html" class="internal-link" target="_self" rel="noopener nofollow">What is the Historical Development of CAR-2-X Technology</a>
<br><br>
<br>Collaborative Work - Guiding and motivating yourself
<br>Introduction to Data Science
<br>Introduction to Programming with Python
<br>Mathematics - Analysis
<br>Statistics - Probability and Descriptive Statistics
<br>Database Modeling and Database Systems
<br>Mathematics - Linear Algebra
<br>Data Science Software Engineering
<br>Machine Learning - Supervised Learning
<br>Machine Learning—Unsupervised Learning and Feature Engineering
<br>Big Data Technologies
<br>Cloud computing
<br>Neural Nets and Deep Learning
<br>Time Series Analysis
<br>Artificial Intelligence
<br>Self-Driving Vehicles
<br>Advanced Data Analysis
<br>Introduction To Data Protection and Cyber Security
]]></description><link>00-index\international-university-(iu).html</link><guid isPermaLink="false">00 - Index/International University (IU).md</guid><pubDate>Wed, 30 Apr 2025 15:24:33 GMT</pubDate></item><item><title><![CDATA[Lehner Electronics]]></title><description><![CDATA[ 
 <br>My first role was as a Project Manager at a cable manufacturing company from December 2019 to<br>
February 2020. Although the position was short-term due to the company’s financial difficulties and non-renewal of my contract, I gained experience managing projects that bridged technical operations with business objectives. My responsibilities included coordinating between engineering teams and management, ensuring project requirements were aligned with production capabilities. My most notable project during this short time period was the design and 3D Modelling of the company renovation. <br>
<br><a data-href="3D Facility Renovation Visualization Using Autodesk Inventor" href="01-entries\1.1-work\lehner-electronics-entries\3d-facility-renovation-visualization-using-autodesk-inventor.html" class="internal-link" target="_self" rel="noopener nofollow">3D Facility Renovation Visualization Using Autodesk Inventor</a> (01.2020 – 02.2020)
]]></description><link>00-index\lehner-electronics.html</link><guid isPermaLink="false">00 - Index/Lehner Electronics.md</guid><pubDate>Thu, 01 May 2025 07:45:38 GMT</pubDate></item><item><title><![CDATA[Marco Treppen]]></title><description><![CDATA[ 
 <br>From April, 2020 to December 2023 I have been employed in a technical role at a company spe-<br>
cializing in staircase manufacturing. While the job title was mechatronics engineer, during my em-<br>
ployment, I primarily served as a project manager for many technical related projects, which is<br>
also how I got my “Engineer” title. Alongside my work, I pursued a bachelor's degree. As my<br>
studies progressed, my professional focus gradually shifted toward data-driven roles, particularly in<br>
areas such as data analysis and marketing. My most notable projects during this time are as follows:<br>
<br><a data-href="Marketing Data Pipeline &amp; Reporting Automation – FB Ads Performance" href="01-entries\1.1-work\marco-treppen-entries\marketing-data-pipeline-&amp;-reporting-automation-–-fb-ads-performance.html" class="internal-link" target="_self" rel="noopener nofollow">Marketing Data Pipeline &amp; Reporting Automation – FB Ads Performance</a> (04.2022 - 12.2023)
<br><a data-href="Social Media Marketing &amp; Brand Revitalization Project" href="01-entries\1.1-work\marco-treppen-entries\social-media-marketing-&amp;-brand-revitalization-project.html" class="internal-link" target="_self" rel="noopener nofollow">Social Media Marketing &amp; Brand Revitalization Project</a> (01.2022 - 12.2023)
<br><a data-href="Creating a staircase configurator with augmented reality function" href="01-entries\1.1-work\marco-treppen-entries\creating-a-staircase-configurator-with-augmented-reality-function.html" class="internal-link" target="_self" rel="noopener nofollow">Creating a staircase configurator with augmented reality function</a> (02.2021 - 12.2021)
<br><a data-href="Excel-Based Pricing Calculator – Workflow Automation Project" href="01-entries\1.1-work\marco-treppen-entries\excel-based-pricing-calculator-–-workflow-automation-project.html" class="internal-link" target="_self" rel="noopener nofollow">Excel-Based Pricing Calculator – Workflow Automation Project</a> (05.2020 - 12.2020)
]]></description><link>00-index\marco-treppen.html</link><guid isPermaLink="false">00 - Index/Marco Treppen.md</guid><pubDate>Thu, 08 May 2025 07:02:18 GMT</pubDate></item><item><title><![CDATA[3D Facility Renovation Visualization Using Autodesk Inventor]]></title><description><![CDATA[ 
 <br><br>During my time as a Project Manager at a cable manufacturing company, I undertook a design-focused side project that aimed to modernize the company's facility planning process. The original renovation concept was limited to 2D technical drawings and architectural documents, which made it difficult for non-technical stakeholders to fully understand spatial layouts and potential design flaws. To bridge this gap, I initiated and executed a 3D modeling project using Autodesk Inventor, with the goal of creating a detailed, accurate, and visually accessible 3D representation of the planned renovations.<br>This model served not only as a powerful communication tool but also as a functional design validation asset. It enabled early detection of spatial conflicts and alignment issues that would have otherwise gone unnoticed until construction, thereby reducing the risk of costly adjustments during implementation.<br><br>
<br>2D to 3D Conversion: Translated architectural blueprints and technical documentation into a precise, to-scale 3D model using Autodesk Inventor.
<br>Modeling &amp; Visualization: Created high-fidelity representations of walls, workstations, machinery, cable routing, and walkways to provide a realistic preview of the post-renovation space.
<br>Design Validation: Simulated spatial relationships and clearance zones to identify potential bottlenecks, layout inefficiencies, or conflicts (e.g., machinery placement obstructing movement or safety pathways).
<br>Stakeholder Communication: Used the 3D model to present the design to management and cross-functional teams, improving understanding and enabling collaborative decision-making.
<br>Iterative Refinement: Incorporated feedback from engineers and project stakeholders to update the model and resolve layout concerns before committing to renovation costs.
<br><br>
<br>Transformed static 2D drawings into an interactive 3D environment, increasing clarity and engagement during design reviews.
<br>Utilized Inventor not just for technical modeling, but as a communication bridge
<br>Introduced design-thinking principles to a traditional industrial environment, encouraging early validation and visual prototyping.
<br>Enabled the team to preemptively identify construction and workflow challenges, improving confidence in the renovation plan.
<br><br>The use of a 1:1 scale 3D visualization significantly improved stakeholder alignment and planning accuracy. The model revealed several overlooked issues, such as insufficient clearance in the production enviroment and conflicts in personnel flow, allowing these to be corrected before physical implementation. This proactive approach helped avoid rework costs and project delays. Despite the short-term nature of the role, the project was impactful and lead to the successful renovation of the facility.]]></description><link>01-entries\1.1-work\lehner-electronics-entries\3d-facility-renovation-visualization-using-autodesk-inventor.html</link><guid isPermaLink="false">01 - Entries/1.1 - Work/Lehner Electronics Entries/3D Facility Renovation Visualization Using Autodesk Inventor.md</guid><pubDate>Wed, 07 May 2025 09:13:26 GMT</pubDate></item><item><title><![CDATA[Creating a staircase configurator with augmented reality function]]></title><description><![CDATA[ 
 <br><br>The starting point of this project was an outdated application form based on 2D sketches and text descriptions. This format often led to miscommunication and uncertainty for clients when visualizing staircase designs. The main objective was to modernize and streamline the request process by introducing a more visual and interactive solution. To meet client expectations, the project aimed to present staircase designs in a realistic, 3D view. The solution would allow users to better understand the final product and interact with it in their own space using Augmented Reality (AR). The goals were to improve customer communication, increase satisfaction, enhance process efficiency, and ultimately drive sales through an upgraded user experience.<br>I led the project from start to finish, taking on both the project management and execution. My responsibilities began with analyzing the existing system and identifying user requirements in close coordination with the leadership team. I also researched suitable tools and developed the technical and financial plan to deliver an AR-based solution. Throughout the project, I managed all phases—from concept development to implementation—ensuring alignment with client needs. I was solely responsible for technical execution and collaborated directly with company decision-makers.<br><img alt="Buchentisch render.png" src="lib\media\buchentisch-render.png"><br>To create accurate 3D models, I used Autodesk Inventor for the initial technical design. Due to Inventor’s limitations in visual presentation, I transferred the models to Blender for enhanced rendering, texturing, and scene creation. I then exported the models for use in Verge3D, a platform chosen for its seamless integration with WordPress, allowing the interactive configurator to be embedded directly on the company website. In Verge3D, I developed both the visual interface and the logic for the configuration form. Customers could select materials, staircase styles, and other options, experiencing the final product in real-time and even in their own space via AR. This end-to-end solution successfully combined visual design with functional interactivity, delivering a significantly improved user journey.<br><br>
<br>Requirements Analysis: Identified key user pain points in the outdated staircase design request process by collaborating with leadership and reviewing customer feedback.
<br>3D Modeling: Created precise staircase models using Autodesk Inventor for technical accuracy, then enhanced them in Blender for high-quality rendering and visual appeal.
<br>Web Integration: Exported interactive 3D models to Verge3D, enabling configuration logic and seamless embedding within a WordPress site.
<br>User Experience Design: Developed an intuitive interface allowing users to customize staircase materials and styles, while visualizing the outcome in real-time through AR.
<br>Augmented Reality Implementation: Enabled customers to place staircase models in their physical environment using mobile AR functionality, enhancing understanding and trust in the product.
<br>Project Leadership: Oversaw the entire lifecycle — from concept, budgeting, and stakeholder communication to technical execution and deployment.
<br><br>
<br>Replaced static 2D forms with a dynamic, user-controlled 3D configurator, resolving major communication gaps between clients and the business.
<br>Pioneered the use of AR in the company’s customer experience, allowing users to preview staircase designs in their own physical spaces.
<br>Led both the technical and strategic sides of the project, combining CAD modeling, 3D rendering, interface design, and workflow integration.
<br>Delivered an interactive tool embedded directly on the company website, fully aligned with marketing and operational goals.
<br><br>The configurator significantly improved the clarity and engagement of the sales process, reducing the number of customer revisions and misunderstandings. By offering an immersive, self-service design experience, the tool led to a measurable increase in customer satisfaction and contributed to a faster decision-making process, accelerating the sales cycle. Internally, it enhanced team efficiency by reducing the time needed for manual clarification and follow-ups. The visual nature of the configurator also elevated the company’s brand perception, showcasing innovation and responsiveness to customer needs, and ultimately positioned the company more competitively in the market.]]></description><link>01-entries\1.1-work\marco-treppen-entries\creating-a-staircase-configurator-with-augmented-reality-function.html</link><guid isPermaLink="false">01 - Entries/1.1 - Work/Marco Treppen Entries/Creating a staircase configurator with augmented reality function.md</guid><pubDate>Wed, 07 May 2025 12:11:14 GMT</pubDate><enclosure url="lib\media\buchentisch-render.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\buchentisch-render.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Excel-Based Pricing Calculator – Workflow Automation Project]]></title><description><![CDATA[ 
 <br><br>The original pricing process for staircase configurations relied on manual calculations, which were time-consuming, inconsistent, and prone to human error. Every customer request required individual effort to estimate costs, which slowed down response times and affected overall accuracy. To resolve this, the goal was to develop a streamlined and reliable cost calculation tool. The company needed a solution that could quickly generate precise staircase price estimates based on various design parameters, while also integrating seamlessly into the existing workflow.<br>As the lead on this project, I was responsible for both the technical development and project coordination. My primary task was to design and build a user-friendly cost calculator in Excel, enabling the business to generate accurate price quotes efficiently. I also led the planning and resource estimation, making sure timelines and project scope were properly defined and managed. I worked closely with stakeholders to analyze the existing manual process and transform it into an automated, modular system. I also conducted internal training to ensure the team could effectively use the new tool.<br><br>
<br>Process Analysis: I started by identifying cost-driving factors such as material and labor inputs. These variables formed the foundation of the calculator’s logic.
<br>Modular Design in Excel: I built the system around a modular structure, breaking down staircase configurations into interchangeable components. This allowed users to input different options (e.g., material types, dimensions) and receive immediate cost estimates.
<br>Automation &amp; Efficiency: The calculator automatically factored in material, labor, and overhead costs. If only one variable changed (e.g., the type of wood), the system adapted the pricing without requiring recalculation of the entire project.
<br>Workflow Integration: Since Excel was already in use, the new solution fit naturally into daily operations. No additional software was required, ensuring easy adoption and minimal disruption.
<br>Team Training: I facilitated onboarding and knowledge transfer sessions so staff could confidently operate the tool going forward.
<br><br>
<br>Led development of a modular Excel-based pricing calculator that replaced error-prone manual calculations for staircase configurations.
<br>Analyzed cost drivers (e.g., material, labor) and transformed them into dynamic inputs within the tool, improving pricing accuracy.
<br>Built a scalable, user-friendly system that integrated seamlessly into existing workflows without requiring new software.
<br>Introduced automation that allowed pricing updates to reflect single-variable changes instantly, eliminating full recalculation cycles.
<br>Managed full project lifecycle — from stakeholder communication and scope definition to technical implementation and team training.
<br>Facilitated internal adoption through training sessions, ensuring long-term usability and knowledge retention.
<br><br>The implementation of the pricing calculator significantly reduced the time needed to generate quotations, cutting the average response time by an estimated 50%. It improved pricing accuracy and consistency, which helped build customer trust and reduced misquotes. Ultimately, the solution contributed to higher operational efficiency, faster sales cycles, and a noticeable improvement in team productivity, all without incurring additional software costs.]]></description><link>01-entries\1.1-work\marco-treppen-entries\excel-based-pricing-calculator-–-workflow-automation-project.html</link><guid isPermaLink="false">01 - Entries/1.1 - Work/Marco Treppen Entries/Excel-Based Pricing Calculator – Workflow Automation Project.md</guid><pubDate>Wed, 07 May 2025 09:12:04 GMT</pubDate></item><item><title><![CDATA[Marketing Data Pipeline & Reporting Automation – FB Ads Performance]]></title><description><![CDATA[ 
 <br><br>This project focused on designing and implementing an automated data pipeline and reporting infrastructure to support performance analysis of Facebook marketing campaigns. As part of a broader data modernization initiative at <a data-href="Marco Treppen" href="00-index\marco-treppen.html" class="internal-link" target="_self" rel="noopener nofollow">Marco Treppen</a>, the goal was to streamline how marketing teams accessed and interpreted campaign metrics. Using Python, Excel, and Power BI, I developed a reporting system that reduced manual processing, enabled consistent cross-departmental access to actionable insights, and improved campaign performance monitoring. The included Power BI dashboard is a visual slice of this larger effort, highlighting a specific cold-audience conversion funnel analysis.<br><br>
<br>
Data Extraction &amp; Transformation:

<br>Source: Facebook Business Manager marketing data
<br>Tools: Python (Pandas, Facebook Graph API), Excel
<br>Processes: Automated data pulls, cleaning (e.g., timestamp normalization, metric alignment), and formatting for dashboard ingestion


<br>
Dashboard Design &amp; Reporting:

<br>Tool: Power BI Desktop
<br>Layout: Funnel visualizations, demographic segmentation, conversion metrics
<br>Metrics: Impressions, CTR (Click-Through Rate), conversion rate, conversions by age and gender


<br>
Delivery &amp; Use:

<br>Dashboard used by marketing and budget teams for weekly and monthly campaign reviews
<br>Supported operational decision-making (targeting, spend allocation, creative performance)


<br><br>
<br>Built a fully automated reporting pipeline, eliminating manual data downloads and copy-paste operations.
<br>Combined Python automation with user-friendly Excel templates for dynamic campaign metric updates.
<br>Developed custom Power BI visuals to highlight conversion funnel drop-offs and audience segment performance.
<br>Enabled faster and more data-driven campaign iteration cycles, resulting in higher ROI and more informed targeting.
<br>Established a reusable framework applicable to other ad platforms (e.g., Google Ads, LinkedIn Campaign Manager).
<br><br>As part of the full-scale internal marketing reporting solution, the Power BI dashboard captured a specific performance scenario: cold audience performance from reach to conversion. This visual report illustrates critical drop-off points and demographic patterns to inform campaign targeting and creative optimization.<br><img alt="Cold Audience to Conversion Funnel.png" src="lib\media\cold-audience-to-conversion-funnel.png"><br>The funnel graphic provides a top-to-bottom view of cold audience engagement. On average 0,5% of the users seeing the ad for the first time clicked the link, and ultimately from these user 1,4% converted, which leads to a final conversion rate 0,1%.This sharp funnel drop-off highlights the difficulty in cold outreach effectiveness, underscoring a key insight: initial targeting and creative resonance with new audiences needed optimization. This funnel acted as a weekly benchmark for evaluating awareness-stage strategies.<br><img alt="Column Chart conversion comparison.png" src="lib\media\column-chart-conversion-comparison.png"><br>Looking at these numbers directly and comparing them with the Facebook standards at the time paints a clearer picture. The average Facebook CTRs typically range from 0.9% to 1.5% across most industries, which likely means the ad creative or targeting isn’t resonating with users. The avergae conversion rate on the other hand is 2% to 10% depending on the industry and the funnel. The conversion rate of 1,4% is not ideal, but both of these numbers can be relativized in terms that high-ticket items like staircases were sold, which usualy perform under the average due to the higher price point. The conversion rate then suggested that it is likely a signal that landing page relevance or user journey flow required improvements. This insight led to A/B testing of landing page layouts and messaging.<br><img alt="Total conversions by age group.png" src="lib\media\total-conversions-by-age-group.png"><br>This column chart ranking showed that for these particular campaigns the top converting age segment was from 25-34 years, closely followed by the 35-44 year age group. This information was particularly crucial with the next graph<br><img alt="Proportional Conversion.png" src="lib\media\proportional-conversion.png"><br>The stacked column chart revealed conversion proportions across both gender and age. This demographic-based insight informed audience prioritization and budget reallocation, directing spend toward adult, male demographics while adjusting messaging tone and visuals to better resonate with them.<br><br>This dashboard is part of a broader data infrastructure I developed, where similar visualizations were created for other metrics and campaign types. The automated system provided a template for future reporting needs and empowered marketing teams to pivot based on evidence-backed decisions rather than gut feeling.]]></description><link>01-entries\1.1-work\marco-treppen-entries\marketing-data-pipeline-&amp;-reporting-automation-–-fb-ads-performance.html</link><guid isPermaLink="false">01 - Entries/1.1 - Work/Marco Treppen Entries/Marketing Data Pipeline &amp; Reporting Automation – FB Ads Performance.md</guid><pubDate>Wed, 07 May 2025 14:58:24 GMT</pubDate><enclosure url="lib\media\cold-audience-to-conversion-funnel.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\cold-audience-to-conversion-funnel.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Social Media Marketing & Brand Revitalization Project]]></title><description><![CDATA[ 
 <br><br>The company, despite having a strong reputation, was experiencing declining sales due to outdated marketing methods and an overreliance on word-of-mouth referrals. Traditional channels such as newspaper advertising were no longer effective or scalable. The objective of this project was to modernize customer acquisition by implementing a data-driven digital marketing strategy. The goal was to build a sustainable and measurable online presence that could drive traffic, improve customer understanding, and increase marketing ROI.<br>While still pursuing my academic degree, I independently led this initiative, taking on both the strategic planning and execution. I leveraged my growing knowledge in data analytics and marketing performance to build a fully data-informed approach, optimizing continuously based on real-time feedback and performance metrics.<br><br>
<br>Digital Strategy Development: Created a modern digital marketing strategy focused on measurable results, targeting the most relevant platforms and audiences.
<br>Platform Selection &amp; Setup: Chose Facebook Business Manager for its robust targeting capabilities and detailed analytics.
<br>KPI Definition &amp; Tracking: Established clear key performance indicators (KPIs) such as CTR, CPC, engagement rate, and conversion metrics to guide decisions.
<br>A/B Testing: Implemented iterative A/B tests across ad creatives and audience segments to identify high-performing content and configurations.
<br>Data Analysis Pipeline: Collected campaign data into spreadsheets and dashboards, applying statistical techniques to evaluate performance and track long-term trends.
<br>Real-Time Optimization: Adapted ad messaging, targeting, and budget allocations on a weekly basis using live performance data.
<br>Team Collaboration: As the campaign scaled, collaborated with a newly formed marketing team to refine and expand the online presence.
<br><br>
<br>Fully self-initiated and executed digital marketing transformation during university studies.
<br>Designed and implemented a data-first social media strategy that replaced outdated marketing methods.
<br>Introduced performance tracking and data analysis pipelines that became integral to marketing decisions.
<br>Applied A/B testing methodology to drive continuous improvement in ad effectiveness and audience targeting.
<br>Helped lay the foundation for sustainable, ROI-driven digital acquisition.
<br><br>The project led to a significant increase in online visibility, higher engagement and conversion rates, and a more effective allocation of marketing budgets. Marketing decisions shifted from intuition-based to data-backed, allowing the company to understand customer behavior more deeply and refine its positioning in the digital space. The integration of real-time analytics resulted in faster iteration cycles and continuous performance improvement. Over time, the strategy proved scalable and was further expanded through collaboration with a dedicated marketing team, solidifying the company's long-term digital capabilities.]]></description><link>01-entries\1.1-work\marco-treppen-entries\social-media-marketing-&amp;-brand-revitalization-project.html</link><guid isPermaLink="false">01 - Entries/1.1 - Work/Marco Treppen Entries/Social Media Marketing &amp; Brand Revitalization Project.md</guid><pubDate>Wed, 07 May 2025 12:11:05 GMT</pubDate></item><item><title><![CDATA[Development of the KAR-MA 4000 Vertical Carousel Storage System]]></title><description><![CDATA[ 
 <br><br>This project involves the design, development, and specification of a vertical carousel storage system—called KAR-MA 4000—intended to store and manage cylindrical workpieces efficiently in a compact, accessible, and user-friendly manner. The system uses rotating storage levels that are moved by a linear actuator, allowing users to retrieve or store parts without manually searching through inventory. The documents collectively define its mechanical structure, construction details, and functional requirements, covering aspects such as load-bearing calculations, safety, environmental suitability, and user interaction. The focus is on reliability, space-saving design, and ease of operation, especially in industrial or workshop settings.<br><br><br>The Requirement Specification sets out the functional expectations, environmental conditions, and safety standards for the system. It includes roles for operation, servicing, and transport, as well as basic details of the control and drive systems. The document ensures that all essential technical and user requirements are clearly defined for development and implementation.<br><br><br>The Static Design Document outlines the mechanical analysis and structural calculations for a vertical carousel magazine intended to store cylindrical components. It considers both fully loaded and asymmetrically loaded scenarios, determining support reactions, bending moments, and shaft stresses. Based on material strength and fatigue limits, the necessary shaft dimensions and bearing requirements are calculated to ensure safe operation under expected loads.<br><br><br>The Construction Document describes the overall design and functionality of the carousel magazine, known as the KAR-MA 4000. It explains how the system organizes and rotates storage levels to access cylindrical parts using a linear actuator and positioning sensors. The document focuses on usability, compactness, and ease of maintenance, with an emphasis on an intuitive control panel and visual accessibility through a transparent panel.<br>]]></description><link>01-entries\1.2-education\htl-eisenstadt-entries\development-of-the-kar-ma-4000-vertical-carousel-storage-system.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/HTL Eisenstadt Entries/Development of the KAR-MA 4000 Vertical Carousel Storage System.md</guid><pubDate>Wed, 07 May 2025 09:16:36 GMT</pubDate></item><item><title><![CDATA[Development of the RE-PORT 5000 Automated Quality Control System]]></title><description><![CDATA[ 
 <br><br>As part of a multidisciplinary final-year engineering project at HTL Eisenstadt, I led the complete design and documentation of the RE-PORT 5000, an automated inspection and sorting machine for food-grade rice cakes. The system was engineered from the ground up, with responsibilities spanning mechanical dimensioning, electrical circuit design, safety protocols, and control system integration.<br>The RE-PORT 5000 uses a multi-stage conveyor system, real-time visual inspection via an industrial camera, and servo-driven rejection mechanics. A custom-built industrial PC interface (with HMI panel) facilitates control, diagnostics, and system status visualization. The project followed industry standards, including CE conformity and DIN EN ISO 7010-compliant labeling and safety systems.<br><br>
<br>Electrical Engineering Design: Developed detailed electrical schematics including control, power, and safety circuits using professional CAD tools. Designed start/stop logic, emergency stop functionality, and system feedback loops.
<br>Component Selection &amp; Integration: Specified and integrated all electrical components, necessary for the functionality of the machine

<br>3 conveyor and servo motors (M1–M3)
<br>Industrial PC with HMI (A2, A3)
<br>Industrial camera (B1) for optical inspection
<br>_Motor protection, control relays, fuses, and transformer systems (F1–F3, Q1–Q4, T1)


<br>Mechanical Engineering Contribution: Calculated mechanical load assumptions and dimensions for conveyor operation, supporting proper sizing of motors and frames.
<br>Control &amp; Safety Circuitry: Designed circuits for safe system startup, shutdown, and emergency handling including pushbuttons, signal lamps (P1–P3), and isolating switches in compliance with CE guidelines.
<br>System Architecture: Created a fully modular architecture that separates mechanical movement, visual inspection, and control units, enabling future adaptability and expandability.
<br>User-Centric Interface Design: Integrated a touchscreen-based HMI for real-time display of operation data, production throughput, and error messages, ensuring usability and traceability
<br>]]></description><link>01-entries\1.2-education\htl-eisenstadt-entries\development-of-the-re-port-5000-automated-quality-control-system.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/HTL Eisenstadt Entries/Development of the RE-PORT 5000 Automated Quality Control System.md</guid><pubDate>Wed, 07 May 2025 09:16:50 GMT</pubDate></item><item><title><![CDATA[Modular Trade Fair Stand Design & Interactive Exhibition Toolkit]]></title><description><![CDATA[ 
 <br><br>As part of my final year at HTL Eisenstadt (Higher Technical College for Mechatronics), I led the development of a next-generation modular trade fair stand—Messestand V2.0—to modernize the school's public presentation at educational events. This project was initiated to replace an outdated system with a scalable, visually cohesive, and technically enhanced exhibition stand.<br>
I served as project team lead, overseeing planning, modeling, and execution while also contributing extensively to the design and technical implementation. My role focused primarily on the design and 3D modeling, however I also was respnsible for working out the giveaways. The booth was designed to be transportable in a station wagon, easily assembled on-site, and aligned with professional trade fair norms (e.g., Messe Wien standards), while being as cost-effective as possible.<br><br>
<br>Team Leadership: Managed the overall project flow, task distribution, and coordination, ensuring timely and high-quality delivery.
<br>Complete 3D Modeling: Designed and built all booth models in Autodesk Inventor, including modular components (booth sizes from 8 m² to 32 m²), custom furniture, media towers, and display structures.
<br>Custom Graphic Design: Created department-themed branding materials (roll-ups, table covers, posters, beachflags) using Adobe Photoshop, reflecting a unified and professional identity.
<br>Multimedia Integration: Integrated monitors, tablets, and a custom-designed media tower to display department-specific videos and presentations.
<br>Compact &amp; Modular Design: Engineered the entire system to be modular, quick to assemble, and compact enough for transportation in a standard vehicle trunk.
<br>Giveaway Concept &amp; Development: Designed physical giveaways like DIY construction kits, stickers, and branded merchandise to engage visitors and promote HTL-Eisenstadt.
<br>User-Centered Approach: Conducted interviews with department heads and analyzed prior setups to align the booth with both educational goals and visitor expectations.
<br>]]></description><link>01-entries\1.2-education\htl-eisenstadt-entries\modular-trade-fair-stand-design-&amp;-interactive-exhibition-toolkit.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/HTL Eisenstadt Entries/Modular Trade Fair Stand Design &amp; Interactive Exhibition Toolkit.md</guid><pubDate>Wed, 07 May 2025 09:18:31 GMT</pubDate></item><item><title><![CDATA[Trennung der Variablen & Variation der Konstanten]]></title><description><![CDATA[ 
 <br>This project focused on solving first-order ordinary differential equations using two fundamental methods: separation of variables and variation of constants. It began with a theoretical introduction to differential equations, followed by a detailed step-by-step explanation of both techniques, including conditions for applicability and examples with full mathematical derivations. The method of separation of variables was used to isolate and integrate functions of xxx and yyy, while variation of constants was applied to solve inhomogeneous linear differential equations based on the corresponding homogeneous solutions. Additionally, historical context was provided through a brief overview of contributions by mathematicians like Euler and Lagrange. The work concluded with reflections on the mathematical and practical relevance of these methods in science and engineering.<br>]]></description><link>01-entries\1.2-education\htl-eisenstadt-entries\trennung-der-variablen-&amp;-variation-der-konstanten.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/HTL Eisenstadt Entries/Trennung der Variablen &amp; Variation der Konstanten.md</guid><pubDate>Wed, 07 May 2025 09:15:19 GMT</pubDate></item><item><title><![CDATA[Airbnb Data Mart Development – SQL & Python Integration]]></title><description><![CDATA[ 
 <br><br>Designed and implemented a fully functional Data Mart simulating an Airbnb-style rental platform, using SQL (MariaDB) and Python. This academic project focused on real-world database development, including data modeling, system logic, normalization, and procedural automation.<br><br>1. Concept Phase:<br>
<br>Defined business logic for a dual-user rental platform (guests and hosts).
<br>Created a detailed ER model and data dictionary based on Airbnb operations.
<br>Requirements gathered for functionalities like booking, payments, reviews, and accommodation listings.
<br>2. Development Phase:<br>
<br>Implemented a normalized database structure with 25+ interconnected tables.
<br>Developed over 30 stored procedures for core operations (user registration, booking, payment confirmation/cancellation, accommodation creation, and review management).
<br>Ensured referential integrity with cascading updates/deletes and complex conditional logic.
<br>Populated test data using both Python scripts and procedure-based SQL inserts.
<br>3. Finalization Phase:<br>
<br>Conducted extensive testing with simulated real-world data (fake addresses, payment data).
<br>Applied tutor feedback iteratively to improve architecture and procedural validation.
<br>Output includes a fully documented GitHub repository with SQL scripts, ER diagrams, and metadata analysis.
<br><br>
<br>Full-stack database implementation using SQL (MariaDB) and phpMyAdmin.
<br>Use of Python for data population and validation scripting.
<br>Built for portability and tested on cross-platform XAMPP environments.
<br>Focused on maintainability, scalability, and real-world use case fidelity.
<br><br>The development of the Airbnb Data Mart followed a structured, real-world project lifecycle consisting of three distinct phases: conceptual design, implementation, and finalization. The project began with an in-depth conceptual phase, where the main objective was to replicate the business logic and core operations of a platform like Airbnb. Functional requirements were defined to support two distinct user groups—guests and hosts—along with entities such as accommodations, reservations, payments, and reviews. A comprehensive Entity-Relationship (ER) model and data dictionary were developed to establish the relationships between these entities, ensuring data normalization up to the third normal form (3NF) to minimize redundancy and improve consistency.<br>ER Model<br><br>Data Dictionary<br><br>In the implementation phase, the logical model was translated into a relational database using MariaDB via phpMyAdmin (XAMPP). A total of 25+ interlinked tables were created to reflect the complexity of the domain. Over 30 stored procedures were written in SQL to automate key operations such as registering users, managing accommodations, processing reservations, handling payments, and creating or deleting reviews. Each procedure included logic for input validation and referential integrity checks, ensuring robust transactional behavior. For example, the GuestBooking procedure ensured that bookings were only accepted if dates were valid, the accommodation was available, and payment methods existed. Custom logic also handled payment confirmations, with expiration logic for cancellations, and conditional deletion of hosts/accommodations only if no active bookings were present.<br><br>The finalization phase involved populating the database with test data using both SQL and Python scripting. Data included simulated user information, addresses, contact details, and financial transactions, some of which were gathered from public datasets or generated with data fabrication tools. During this phase, all procedures were rigorously tested in realistic scenarios, and refinements were made based on tutor feedback—such as optimizing procedure logic, enforcing stricter foreign key constraints, and improving data traceability. The completed project is fully documented and hosted on GitHub.<br><br>Github Repository: <a rel="noopener nofollow" class="external-link" href="https://github.com/sanax-997/project_airbnb_data_mart.git" target="_blank">https://github.com/sanax-997/project_airbnb_data_mart.git</a>]]></description><link>01-entries\1.2-education\iu-entries\airbnb-data-mart-development-–-sql-&amp;-python-integration.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Airbnb Data Mart Development – SQL &amp; Python Integration.md</guid><pubDate>Wed, 07 May 2025 12:30:52 GMT</pubDate></item><item><title><![CDATA[Architecture Proposal for Constructing a Data Warehouse]]></title><description><![CDATA[ 
 <br>]]></description><link>01-entries\1.2-education\iu-entries\architecture-proposal-for-constructing-a-data-warehouse.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Architecture Proposal for Constructing a Data Warehouse.md</guid><pubDate>Wed, 07 May 2025 12:20:14 GMT</pubDate></item><item><title><![CDATA[Demand Forecasting Model for Public Transport – Predictive Time Series Analysis in Python]]></title><description><![CDATA[ 
 <br><br>This project focuses on building a time-series forecasting model to predict hourly taxi demand across key locations in New York City, using the "Uber Pickups in New York" dataset. Designed within the CRISP-DM framework, the project combines exploratory data analysis (EDA), geographic clustering, and SARIMA modeling to offer insights into transportation demand patterns. The ultimate goal is to support logistics optimization, reduce wait times, and improve dispatch efficiency through data-driven predictions.<br><br> 1. Methodology &amp; Framework<br>
<br>Followed the CRISP-DM lifecycle: Business Understanding → Data Preparation → Modeling → Evaluation → Deployment Strategy.
<br>Defined KPIs (e.g., prediction accuracy) and KRIs (e.g., 15% revenue increase target).
<br>Data sourced from Kaggle’s Uber Pickups in NYC (2019), consisting of six months of clean, well-structured time and location data.
<br>2. Data Engineering &amp; Preprocessing<br>
<br>Generated time-based features: hour of day, weekday/weekend, holiday indicators.
<br>Employed clustering to create location-based features (e.g., Midtown, Central Park, Brooklyn).
<br>Data quality was exceptionally high, allowing for rapid progression through the ETL pipeline.
<br>3. Model Development &amp; Evaluation<br>
<br>Chose SARIMA (Seasonal Autoregressive Integrated Moving Average) to capture weekly seasonality and monthly demand trends.
<br>Model training included auto_arima parameter tuning using the pmdarima library.
<br>Conducted location-specific forecasting and compared predicted vs. actual pickup volumes.
<br>Performance limitations emerged due to short dataset duration (6 months) and underrepresentation of holiday effects.
<br><br>
<br>Applied SARIMA time-series modeling for spatial-temporal forecasting of public transport demand.
<br>Identified strong weekly cycles and a 2-month seasonality trend in NYC taxi data.
<br>Integrated geospatial clustering to represent key high-demand pickup areas.
<br>Proposed a GUI-based prediction interface for operations teams to input date, time, and location and receive demand forecasts in real time.
<br>Designed a Git-based project structure including modular directories for raw/processed data, code, visualizations, deployment scripts, and documentation.
<br>Introduced a plan for automated retraining and continuous monitoring using a feedback loop and anomaly detection.
<br><br>The project began with a clear business objective: to forecast hourly demand for taxi pickups in New York City in order to optimize vehicle dispatching and resource allocation. The modeling process followed the CRISP-DM methodology, which included defining performance indicators such as prediction accuracy and a key result area aiming for a 15% improvement in fleet efficiency. Data was sourced from the Uber Pickups NYC dataset (April–September), which offered granular timestamp and location details necessary for high-resolution forecasting.<br>To understand short-term patterns in demand, a time-of-day distribution chart was created. The visualization revealed two distinct peaks in pickup activity: a smaller spike around 8:00 AM likely linked to commuting, and a much more pronounced peak around 6:00 PM associated with evening activity and return commutes. <br><img alt="Distribution of Pickups Throughout the Day.jpg" src="lib\media\distribution-of-pickups-throughout-the-day.jpg"><br>These patterns were consistent across weekdays and weekends but differed in magnitude, making time a primary feature for predictive modeling.<br><img alt="Proportional Pickups by Time of Day from April to September 2014.jpg" src="lib\media\proportional-pickups-by-time-of-day-from-april-to-september-2014.jpg"><br>A separate grouped bar chart was used to compare weekday and weekend pickup volumes. The data showed that weekends consistently exhibited higher demand variability, while weekday pickups followed more rigid patterns—especially around business hours. <br><img alt="Number of Pickups per Weekday.jpg" src="lib\media\number-of-pickups-per-weekday.jpg"><br>This suggested that weekday-weekend segmentation was a valuable input for the model and could improve predictive accuracy if incorporated as a categorical variable.<br><img alt="Average Pickups Weekday vs Weekend.jpg" src="lib\media\average-pickups-weekday-vs-weekend.jpg"><br>Next, a bar chart visualized month-to-month demand to detect seasonal trends. A gradual increase in pickups was visible from April through July, with a slight dip in August and September. <br><img alt="Number of Pickups in NYC from April to October 2014.jpg" src="lib\media\number-of-pickups-in-nyc-from-april-to-october-2014.jpg"><br>This was then further confirmed using a simple linear regression. This cyclical behavior indicated seasonality on a 2–3 month scale, reinforcing the decision to use SARIMA, a model specifically designed to account for both trend and seasonality in time-series forecasting.<br><img alt="Linear Correlation of Average Pickups in NYC from April to October 2014.jpg" src="lib\media\linear-correlation-of-average-pickups-in-nyc-from-april-to-october-2014.jpg"><br>To make the model spatially aware, pickup locations were clustered into zones using geographic coordinates. Overall three different graphs where employed to gain a comprehensive understanding. A proportional bar chart, to inspect if there were any changes pickup regions across the year.  <br><img alt="Proportional Pickups by Location from April to September in NYC 2014.jpg" src="lib\media\proportional-pickups-by-location-from-april-to-september-in-nyc-2014.jpg"><br>A normal bar chart was then applied to rank the different regions to determine the largest hotspots and the total amount of pickups.<br><img alt="Number of Pickups by Location in NYC from April to September 2014.jpg" src="lib\media\number-of-pickups-by-location-in-nyc-from-april-to-september-2014.jpg"><br>At last a map was employed to display these clusters, which included known hotspots such as Midtown Manhattan, JFK Airport, Brooklyn, and Central Park. These clusters became a categorical feature, allowing for location-specific demand forecasting. <br><img alt="Map.jpg" src="lib\media\map.jpg"><br>One of the project's core deliverables was a location-specific forecast. For the Brooklyn cluster, a SARIMA model was trained using 4 months of historical data. The resulting plot showed a close alignment between predicted and actual values, particularly in terms of daily cycles. <br><img alt="Brooklyn Pickups with Predictions.jpg" src="lib\media\brooklyn-pickups-with-predictions.jpg"><br>Some divergence occurred during unexpected spikes, attributed to local events or holidays not fully represented in the dataset. Nevertheless, the model achieved high short-term predictive accuracy, validating the methodological approach. However, due to insufficent data, the model showcased limitations in capturing the longer seasonality component spanning across years.<br><img alt="Brooklyn Pickup Predictions.jpg" src="lib\media\brooklyn-pickup-predictions.jpg"><br>In addition, pickup volumes on public holidays were compared with average workday activity. A bar chart revealed significant dips or irregularities in holiday behavior, particularly on dates like the 4th of July. However, due to the dataset only covering 6 months, the number of holidays was too small to train a separate holiday-aware model. This reinforced a future recommendation to extend the training window to a full year, allowing for more granular calendar feature engineering.<br><img alt="Average Pickups Holiday vs Workday.jpg" src="lib\media\average-pickups-holiday-vs-workday.jpg"><br><br>Github Repository: <a rel="noopener nofollow" class="external-link" href="https://github.com/sanax-997/demand_forecasting_model_for_public_transport.git" target="_blank">https://github.com/sanax-997/demand_forecasting_model_for_public_transport.git</a>]]></description><link>01-entries\1.2-education\iu-entries\demand-forecasting-model-for-public-transport-–-predictive-time-series-analysis-in-python.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Demand Forecasting Model for Public Transport – Predictive Time Series Analysis in Python.md</guid><pubDate>Wed, 07 May 2025 12:28:11 GMT</pubDate><enclosure url="lib\media\distribution-of-pickups-throughout-the-day.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\distribution-of-pickups-throughout-the-day.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Enhancement of an Online Retailer's Data Quality Management Capabilities]]></title><description><![CDATA[ 
 <br>]]></description><link>01-entries\1.2-education\iu-entries\enhancement-of-an-online-retailer&apos;s-data-quality-management-capabilities.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Enhancement of an Online Retailer&apos;s Data Quality Management Capabilities.md</guid><pubDate>Wed, 07 May 2025 12:15:51 GMT</pubDate></item><item><title><![CDATA[Ethical Prinicples of a Global Company]]></title><description><![CDATA[ 
 <br>]]></description><link>01-entries\1.2-education\iu-entries\ethical-prinicples-of-a-global-company.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Ethical Prinicples of a Global Company.md</guid><pubDate>Wed, 07 May 2025 12:15:23 GMT</pubDate></item><item><title><![CDATA[Geo-Spatial Energy Analysis – Interactive Data Visualization of Global Energy Trends]]></title><description><![CDATA[ 
 <br><br>This project focuses on creating a data-driven narrative around global energy production and consumption, highlighting the shift toward renewable energy sources. Leveraging the comprehensive OWID Energy dataset, the project visualizes the intersection of energy use, economic growth, and greenhouse gas emissions through an interactive storytelling platform. Combining Python (Pandas, Plotly) with principles of visual storytelling, the analysis reveals key regional disparities, trends, and correlations—empowering informed decisions on energy sustainability and climate responsibility.<br><br> 1. Data Acquisition &amp; Cleaning<br>
<br>
Sourced data from the OWID (Our World in Data) Energy dataset, which covers global energy metrics from 1900–2022.

<br>
Preprocessed raw data to extract relevant variables such as electricity source shares, GDP, CO₂ emissions, and country-level energy production.

<br>
Cleaned and structured the dataset to ensure consistency across time series and categorical columns.
2. Visualization Tools &amp; Techniques

<br>
Used Pandas for data manipulation and Plotly for creating interactive, multi-dimensional visualizations.

<br>
Designed responsive visualizations including:

<br>Geo-spatial choropleth maps.
<br>Pie charts (energy mix).
<br>Area charts (historical growth).
<br>Bar and grouped bar charts (regional comparisons).
<br>Regression plots (GDP vs. energy use).


<br>
Applied color theory to encode meaning (e.g., green for renewables, red for fossil, brown for emissions).
3. Interactivity and Design

<br>
Built a dynamic system where visual elements interact with each other—e.g., clicking a country on the map highlights that country in all related charts.

<br>
Implemented linked selection and filtering across graphs to enhance user experience.

<br>
Followed data storytelling principles from literature to balance clarity, engagement, and technical depth.

<br><br>
<br>Developed an interactive dashboard that links geo-spatial maps with analytical visualizations for deeper contextual understanding.
<br>Provided a comprehensive analysis of global energy patterns, with a special focus on:

<br>Renewable energy growth vs. fossil fuel dominance.
<br>Continental energy disparities.
<br>Top CO₂ emitters and their economic profiles.


<br>Demonstrated how GDP strongly correlates with energy use, especially in countries like China, the U.S., and India.
<br>Delivered a visually intuitive tool to policymakers and educators for exploring energy data from multiple dimensions.
<br>Reinforced technical and creative decisions using academic literature and principles of effective visualization.
<br><br>The project began with a dual objective: to explore the impact of legislation and social movements on the global energy landscape, and to use interactive data visualization to tell that story. After defining the analytical scope, the OWID Energy dataset was selected due to its breadth and reliability. The dataset was examined, cleaned, and structured using Pandas, enabling the foundation for insightful visual analysis.<br>The initial phase focused on visualizing global energy consumption using a choropleth map, with countries colored based on Terra-Watt hour usage. This map highlighted disparities, such as high usage in China and the U.S., and minimal usage in many African nations. <br><img alt="Energy Consumption Disparities.png" src="lib\media\energy-consumption-disparities.png"><br>The project then transitioned to temporal trends, using area charts to show how electricity generation has evolved since 1985—revealing fossil fuels’ continued dominance but also the exponential rise of renewable energy.<br><img alt="World Electricity Sources.png" src="lib\media\world-electricity-sources.png"><br>Subsequent analyses addressed the energy source mix by continent, exposing regional dependencies and leaders in renewable adoption (notably South America). <br><img alt="Share of Electricity Sources by Contintent.jpg" src="lib\media\share-of-electricity-sources-by-contintent.jpg"><br>The project also ranked countries by greenhouse gas emissions, with China standing out as the top contributor—producing more emissions than the next nine countries combined.<br><img alt="Top 10 Countries with the Highest Greenhouse Gas Emissions.png" src="lib\media\top-10-countries-with-the-highest-greenhouse-gas-emissions.png"><br>Further, the correlation between GDP and energy consumption was analyzed through regression plots, affirming that economic growth is a powerful driver of energy demand. China, India, and the U.S. demonstrated this pattern most clearly, reinforcing the idea that sustainability strategies must consider economic trajectories.<br><img alt="Scatter Plot of Primary Energy Consumption.png" src="lib\media\scatter-plot-of-primary-energy-consumption.png"><br>The final section addressed design methodology. Every creative decision—from color schemes (e.g., red = fossil fuels, green = renewables) to chart type selection—was intentional and literature-backed. The visual system ensured that users could intuitively navigate complex datasets. Most notably, the interactivity allowed users to select countries on the map, automatically updating all other visualizations for a richer, personalized data exploration experience.<br><img alt="Showacing Graph Interactivity.png" src="lib\media\showacing-graph-interactivity.png"><br>In conclusion, the project offered both a present-state analysis and forward-looking insights into energy use, while contributing to the broader discourse on interactive data storytelling. It showcased the potential of data visualization not just to inform—but to engage and empower.<br><br>Github Repository: <a rel="noopener nofollow" class="external-link" href="https://github.com/sanax-997/exploratory_data_analysis_and_visualization.git" target="_blank">https://github.com/sanax-997/exploratory_data_analysis_and_visualization.git</a>]]></description><link>01-entries\1.2-education\iu-entries\geo-spatial-energy-analysis-–-interactive-data-visualization-of-global-energy-trends.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Geo-Spatial Energy Analysis – Interactive Data Visualization of Global Energy Trends.md</guid><pubDate>Wed, 07 May 2025 12:28:02 GMT</pubDate><enclosure url="lib\media\energy-consumption-disparities.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\energy-consumption-disparities.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Habit Tracker – A Web-Based Habit Management & Analytics Tool in Python]]></title><description><![CDATA[ 
 <br><br>The Habit Tracker is a full-stack Python web application designed to help users track, manage, and analyze recurring habits through a clean web interface. Developed using Flask, this project showcases object-oriented and functional programming principles while integrating local JSON-based data storage, form validation, and multi-user support. The application allows users to register, log in, create and check off habits, and view analytics on performance—all with data persisted across sessions. This project served as a practical exercise in building end-to-end web applications and deepened my fluency in Python, Flask, and web development best practices.<br><br>1. Conception Phase:<br>
<br>
Defined the core architecture using Object-Oriented Programming (OOP) and Functional Programming (FP) paradigms.

<br>
Outlined the separation of concerns via two core modules:

<br>habits.py: Defines the Habit class with methods for creation, checking, and deletion.
<br>analytics.py: Functional module providing insights such as longest streaks or habits by periodicity.


<br>
Designed a user flow diagram and program architecture diagram for client-server interaction.

<br>
Chose Flask for the backend and HTML/CSS (with Bootstrap) for the frontend interface, hosted locally.
2. Development Phase:

<br>
Integrated Flask to serve as a backend router and expose methods through a web-based UI.

<br>
Built and styled HTML templates using Bootstrap, and implemented routing logic with Jinja2 templating.

<br>
Developed a user authorization system with registration, login/logout functionality using hashed passwords (via werkzeug).

<br>
Implemented file-based local storage using JSON files, managing individual user data securely and efficiently with Python’s os module.

<br>
Completed development of habit management features: create, check, and delete—including automatic streak tracking using datetime.

<br>
Added advanced analytics: view habits with the same periodicity, longest current streaks, and historical bests—all written as side-effect-free functions.

<br>
Developed a dual-part testing suite:

<br>Back-end tests using Python’s unittest to ensure correct server responses.
<br>Front-end tests using Selenium to simulate user interactions and validate UI behavior.


<br>3. Finalization Phase:<br>
<br>Refined architecture based on iterative feedback, improving modularity and readability.
<br>Expanded on UX elements (input validation, error handling, default user setup).
<br>Deployed a working demo with multiple users, default data, and a fully styled interface.
<br><br>
<br>Full-stack Python web application built entirely from scratch.
<br>Combines OOP for data modeling (habits) with FP for analytics (pure functions).
<br>Local JSON file-based storage (instead of databases) to emphasize direct Python data handling.
<br>Built-in authentication system and streak-tracking logic based on periodicity and dates.
<br>Extensive testing coverage for both back-end and front-end logic.
<br>Features a responsive web interface powered by Flask, Bootstrap, and Jinja2.
<br><br>The development of the Habit Tracker began with the conception phase, where the program's architecture was first envisioned. The application was broken down into modular components—habits.py handling the Habit class and all related operations, and analytics.py implementing functional routines for analyzing user data. To deliver a user-friendly interface, Flask was chosen to route backend functions to a web-based frontend, styled using Bootstrap. Diagrams were sketched to map out user flow and client-server interactions, and local JSON files were selected over SQL for lightweight storage and easier prototyping.<br>User Flow Chart<br><br>Class Diagram<br><br>During the development phase, a minimal viable Flask app was set up to confirm client-server routing. After validating functionality, HTML templates and form logic were added, along with secure user registration and login mechanisms using hashed passwords. The Habit class was implemented with features to create, check, and delete habits, and additional functionality tracked streaks using Python’s datetime module. JSON was used to persist all user and habit data per session. As functionality matured, the analytics module was integrated, enabling users to explore patterns in their habit history. Finally, unit tests and automated UI tests using Selenium ensured that both backend logic and frontend user interactions performed reliably.<br><br>In the finalization phase, code was iteratively refined based on tutor feedback, with particular focus on structure, code reuse, and performance. Enhancements included improvements to the authentication flow, better form validation, and UI responsiveness. The application was fully documented, packaged, and accompanied by diagrams and test logs. The result was a modular, well-tested Python web app that demonstrated proficiency across backend development, frontend templating, data storage, and automated testing.<br><br>Github Repository: <a rel="noopener nofollow" class="external-link" href="https://github.com/sanax-997/oofpp_habits_project.git" target="_blank">https://github.com/sanax-997/oofpp_habits_project.git</a>]]></description><link>01-entries\1.2-education\iu-entries\habit-tracker-–-a-web-based-habit-management-&amp;-analytics-tool-in-python.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Habit Tracker – A Web-Based Habit Management &amp; Analytics Tool in Python.md</guid><pubDate>Wed, 07 May 2025 12:29:24 GMT</pubDate></item><item><title><![CDATA[How to Deal with Security and Privacy Threats in the Context of Data Science]]></title><description><![CDATA[ 
 <br>]]></description><link>01-entries\1.2-education\iu-entries\how-to-deal-with-security-and-privacy-threats-in-the-context-of-data-science.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/How to Deal with Security and Privacy Threats in the Context of Data Science.md</guid><pubDate>Wed, 07 May 2025 12:16:02 GMT</pubDate></item><item><title><![CDATA[Image Classification for a Refund Department – End-to-End ML Deployment in Python]]></title><description><![CDATA[ 
 <br><br>This project delivers a complete machine learning-powered software solution for automating the classification of returned fashion items based on product images. Built for a sustainable online clothing platform experiencing scaling challenges, the goal was to streamline the refund department's workflow by replacing manual classification with an AI model. The system integrates image preprocessing, classification using a deep learning model (VGG19), and full-stack deployment using Django. Emphasis was placed not only on model performance but also on enabling real-world production use via a custom-built client-server application.<br><br> 1. Machine Learning Model<br>
<br>
Chose VGG19 (Keras) as the backbone model due to its strong performance on medium-sized datasets and structural simplicity.

<br>
Dataset: 28,000 images (training/testing), split across 4 categories (Bags, Boots, Tops, Trousers).

<br>
Preprocessing included rescaling images to 32x32 grayscale for compatibility.

<br>
Model fine-tuned with 2 dense layers (512 units each), achieving 99.8% training accuracy and 99.58% validation accuracy.

<br>
Model trained using Kaggle, leveraging its GPU-powered environment for efficiency.
2. Data Pipeline and Training

<br>
Raw data provided in .ubyte format; developed a Python script to convert and organize images into structured folders.

<br>
Dataset divided into train/test directories for balanced class representation.

<br>
Training pipeline included augmentation, resizing, and batch processing.
3. Full-Stack Deployment

<br>
Developed a Django-based server with API endpoints for receiving image data, converting strings to images, and returning category predictions.

<br>
Built a Python client application that:

<br>Monitors incoming refund images.
<br>Batches and sends them overnight via HTTP POST.
<br>Parses JSON responses and handles sorting based on classification.


<br>
Conversion between image data and string formats ensured compatibility for transmission.

<br><br>
<br>Delivered a production-grade ML pipeline: data acquisition → model training → deployment → automation.
<br>Achieved real-time image classification and dynamic image sorting based on server response.
<br>Implemented robust client-server communication using HTTP and JSON, with string-based image transmission.
<br>Created a modular client script with adjustable runtime parameters and folder paths.
<br>Used pipenv for virtual environment and dependency management, ensuring ease of installation and reproducibility.
<br>Hosted full project with documentation and user manual on GitHub.
<br><br>The project started by identifying a real-world need: refund departments overwhelmed by manual sorting of returned products. The first task was to select a robust image classification model, for which VGG19 (via Keras) was chosen due to its reliability on medium-complexity image datasets. The dataset—roughly 30,000 low-res fashion images—was cleaned, converted from byte format, and sorted into training and testing folders using a custom Python script.<br>With the dataset prepared, the model was trained on Kaggle to leverage cloud GPU resources. Images were resized and normalized to meet VGG19 requirements, and the model was fine-tuned to the classification task using additional dense layers. The training process delivered exceptional accuracy metrics, making the model ready for deployment.<br>Next, the project shifted to system integration. A Django-based server was developed to host the trained model and expose a REST API for classification requests. On the other side, a Python client script was built with features to batch images, convert them to string format, and send them nightly at 23:00 via POST requests. Upon receiving a response with category predictions, the client renamed and moved the files into appropriate folders based on a naming convention, maintaining clear organization for downstream handling.<br>The final part of the project included a live demonstration, showing end-to-end functionality from launching the server (py manage.py runserver) and sending test images via the client, to observing sorted and renamed output files. All components were wrapped in pipenv-managed environments for portability, and a detailed manual ensures reproducibility. The system is designed to be modular, scalable, and adaptable to additional categories or future ML model upgrades.<br><br><br>Github Repository: <a rel="noopener nofollow" class="external-link" href="https://github.com/sanax-997/project_from_model_to_production.git" target="_blank">https://github.com/sanax-997/project_from_model_to_production.git</a>]]></description><link>01-entries\1.2-education\iu-entries\image-classification-for-a-refund-department-–-end-to-end-ml-deployment-in-python.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Image Classification for a Refund Department – End-to-End ML Deployment in Python.md</guid><pubDate>Wed, 07 May 2025 12:27:53 GMT</pubDate></item><item><title><![CDATA[Introduction to Academic Work]]></title><description><![CDATA[ 
 <br>]]></description><link>01-entries\1.2-education\iu-entries\introduction-to-academic-work.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Introduction to Academic Work.md</guid><pubDate>Wed, 07 May 2025 12:11:24 GMT</pubDate></item><item><title><![CDATA[Mental Health in Technology-Related Jobs – An Exploratory Data Science Case Study]]></title><description><![CDATA[ 
 <br><br>This data science case study explores mental health patterns in technology-related professions using survey data. The project involved comprehensive data preprocessing, transformation, dimensionality reduction, and clustering to uncover psychological trends and demographic vulnerabilities. The goal was not only to clean and prepare raw survey data but to extract meaningful insights that could inform real-world mental health initiatives in tech workplaces. Conducted with Python, the project demonstrates expertise in feature engineering, unsupervised learning, and cluster analysis techniques, with findings grounded in rigorous preprocessing logic and visual interpretability.<br><br>1. Feature Preprocessing &amp; Engineering<br>
<br>Loaded and inspected raw survey data to assess structure and quality.
<br>Conducted extensive feature cleaning, removing columns with high cardinality or &gt;30% missing values.
<br>Segmented and retained relevant records focused on technology-related employment.
<br>Cleaned missing values using:

<br>Logical assumptions based on question context.
<br>Mode imputation where assumptions failed.


<br>Transformed categorical features into numerical form using:

<br>Binary encoding for nominal data.
<br>Ordinal encoding for ordered responses.
<br>One-hot encoding for multi-category features.


<br>2. Dimensionality Reduction<br>
<br>Applied Principal Component Analysis (PCA) to compress feature space but found it ineffective for categorical data.
<br>Switched to Multiple Correspondence Analysis (MCA), which provided significantly better cluster separability and interpretability for survey-based features.
<br>3. Clustering &amp; Analysis<br>
<br>Used K-Means clustering to identify hidden groupings in the data.
<br>Determined the optimal number of clusters using:

<br>Elbow Method (based on WCSS).
<br>Silhouette Score.


<br>Performed cluster analysis across different demographic splits:

<br>Employed vs. self-employed.
<br>Diagnosed vs. non-diagnosed individuals.


<br>Analyzed polar charts to interpret feature impact within clusters.
<br><br>
<br>Demonstrated expertise in feature preprocessing for categorical, incomplete, and survey-based data.
<br>Applied MCA for dimensionality reduction—a technique well-suited for non-numeric variables and widely used in social science analytics.
<br>Performed unsupervised machine learning with K-Means, enabling non-predictive insight into trends.
<br>Revealed strong relationships between mental health and variables such as gender identity, job role, employment status, traumatic experience, and geographic location.
<br>Produced actionable insights for designing preventive mental health programs in tech organizations.
<br><br>The project began with loading and exploring raw survey data, assessing the types and completeness of features. A large portion of the dataset was categorical and contained missing or ambiguous values, which made feature cleaning the first and most critical step. The general cleaning phase involved dropping columns with high cardinality or excessive missing data, such as open-ended text responses and overly granular location fields. More nuanced missing data, especially those tied to conditional logic (e.g., “employer” questions only relevant to employed individuals), led to the segmentation of the dataset into multiple subframes, allowing for more precise imputations and retention of valuable information. <br>After the data was clean, a multi-tiered transformation pipeline converted categorical data into numerical format. Binary encoding was used for dichotomous variables, ordinal mapping for ranked responses, and one-hot encoding for features with multiple categorical values. These transformations enabled compatibility with machine learning models.<br>Next came dimensionality reduction, a key step for high-dimensional survey data. PCA was initially tested, but performed poorly with categorical input. After further research, Multiple Correspondence Analysis (MCA) was chosen as a better alternative. MCA allowed the reduction of feature space to 2–3 dimensions while preserving enough variance to visually identify meaningful clusters.<br>Multiple Correspondence Analysis<br><img alt="Multiple Correspondence Analysis.png" src="lib\media\multiple-correspondence-analysis.png"><br>With a manageable feature space, K-Means clustering was applied. The optimal number of clusters (three) was chosen based on both the Elbow Method and Silhouette Score. <br><img alt="Elbow Score and Silhouette Plot.png" src="lib\media\elbow-score-and-silhouette-plot.png"><br>The clustering revealed significant patterns across different groups. For instance, self-employed individuals and those identifying outside the male/female binary had a higher prevalence of mental health diagnoses. Job type also played a role: front-end developers and dev evangelists showed elevated mental health concerns compared to back-end developers and system administrators.<br><img alt="K-means and Polar Clustering.png" src="lib\media\k-means-and-polar-clustering.png"><br>Visuals such as polar plots and scatter plots of reduced data provided interpretability of each cluster’s composition. These findings led to targeted recommendations for mental health support programs: focusing on at-risk groups (e.g., non-binary individuals, self-employed workers), providing educational resources, and assessing organizational openness to mental health discussions.<br><br>Github Repository: <a rel="noopener nofollow" class="external-link" href="https://github.com/sanax-997/mental_health_analysis.git" target="_blank">https://github.com/sanax-997/mental_health_analysis.git</a>]]></description><link>01-entries\1.2-education\iu-entries\mental-health-in-technology-related-jobs-–-an-exploratory-data-science-case-study.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Mental Health in Technology-Related Jobs – An Exploratory Data Science Case Study.md</guid><pubDate>Wed, 07 May 2025 12:27:45 GMT</pubDate><enclosure url="lib\media\multiple-correspondence-analysis.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\multiple-correspondence-analysis.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[NLP Topic Modeling of Customer Complaints – A Comparative Approach Using LDA and BERTopic]]></title><description><![CDATA[ 
 <br><br>This project focuses on analyzing customer complaints using Natural Language Processing (NLP) to uncover the most frequent topics in unstructured text data. Using the Comcast Consumer Complaints dataset from Kaggle, the project implements a comparative approach between traditional TF-IDF + Latent Dirichlet Allocation (LDA) and a modern BERT-based model via the BERTopic library. The objective is not only to extract key issues voiced by customers but also to evaluate how different modeling techniques reveal insights in varying levels of granularity. This dual-pipeline framework provides a hands-on understanding of the evolution in topic modeling methodologies.<br><br> 1. Data Collection and Preprocessing<br>
<br>
Dataset: Comcast Consumer Complaints (Kaggle)

<br>
Preprocessing: Tokenization, spell-checking, stopword removal, lemmatization

<br>
Enrichment: N-gram creation for contextual depth
2. Topic Modeling Methods

<br>
Traditional Method: TF-IDF vectorization + LDA (using scikit-learn)

<br>
Modern Method: BERT embeddings + c-TF-IDF (using BERTopic and Hugging Face Transformers)

<br>3. Visualization and Comparison<br>
<br>pyLDAvis for LDA topic interpretation
<br>BERTopic's LDAvis-based visualization for transformer model
<br>Topic distance maps to determine number and separation of topics
<br><br>
<br>Implemented and compared two end-to-end NLP topic modeling pipelines.
<br>Demonstrated clear differences in model capabilities—LDA yielded 4 general topic clusters, while BERTopic uncovered 6 more nuanced clusters.
<br>Ensured pipeline uniformity through shared preprocessing and vocabulary, enabling fair comparative analysis.
<br>Used interactive topic visualizations to enhance interpretability and provide a user-friendly analysis output.
<br>Built a functional application interface that allows users to input text and choose their preferred modeling approach.
<br>Gained practical experience in text vectorization, unsupervised learning, and transformer-based NLP models.
<br><br>The project began with the goal of identifying and analyzing major topics found within consumer complaints. After reviewing multiple publicly available datasets, the Comcast Consumer Complaints dataset from Kaggle was selected for its relevant content and structured format. It included columns for author, date, rating, and—most critically—open-ended complaint text. Initial analysis revealed that while the dataset was high quality overall, the text data required extensive preprocessing for use in topic modeling.<br>The first stage of development involved preparing the raw text for analysis. This included tokenization, spell correction, stopword removal, and lemmatization, all performed using the NLTK library. The cleaned tokens were then enriched using n-gram detection, adding multi-word expressions to the vocabulary. This ensured that both pipelines could detect common phrases relevant to telecom complaints (e.g., “billing issue”, “cable outage”).<br>Following preprocessing, the text was fed into two separate topic modeling pipelines for comparison. The first pipeline followed a classic approach: text was transformed into TF-IDF vectors and passed into a Latent Dirichlet Allocation (LDA) model, using scikit-learn. The output topics were then visualized with pyLDAvis, which provided an intertopic distance map—a key tool for determining the optimal number of topics. Four non-overlapping clusters emerged, revealing broad themes like billing disputes and service failures.<br>The second pipeline utilized BERTopic, a more modern framework that combines BERT embeddings with class-based TF-IDF (c-TF-IDF) for topic modeling. This method offered deeper contextual understanding and automatically calculated an optimal number of clusters. The resulting visualization displayed six distinct topic areas, indicating that the transformer model was able to capture subtler patterns and distinctions within the complaints. Topics such as equipment installation issues and specific service failures emerged clearly.<br>A side-by-side comparison of the LDA and BERTopic outputs showed that while LDA was effective in surfacing generalized themes, BERTopic provided more specific and nuanced categorization. Both pipelines used a similar visualization framework (LDAvis-based), making it easy to compare topic separation, keyword clarity, and cluster density. <br><img alt="pyLDAvis visualization v2.jpg" src="lib\media\pyldavis-visualization-v2.jpg"><br>It was clear from the intertopic distance maps that BERTopic captured a greater range of consumer concerns—a reflection of its ability to model language context more richly via transformer embeddings.<br><img alt="BERTopic.jpg" src="lib\media\bertopic.jpg"><br>The final phase of the project focused on building an end-to-end application structure. The result was a modular NLP tool that allows a user to input raw text and select between LDA and BERTopic for analysis. The output is an interactive HTML visualization of the modeled topics. This not only allows for insightful exploration of the data, but also helps users interpret the semantic structure of consumer concerns in a transparent and accessible way.<br>Overall, the project achieved its goals of demonstrating two powerful topic modeling techniques, comparing their outcomes in detail, and building a functional NLP tool. It provided practical experience with both traditional vector-based methods and state-of-the-art transformer approaches, along with insights into their strengths, limitations, and optimal use cases in the context of customer complaint analysis.<br><br>Github Repository: <a rel="noopener nofollow" class="external-link" href="https://github.com/sanax-997/sentiment_analysis_of_customer_reviews.git" target="_blank">https://github.com/sanax-997/sentiment_analysis_of_customer_reviews.git</a>]]></description><link>01-entries\1.2-education\iu-entries\nlp-topic-modeling-of-customer-complaints-–-a-comparative-approach-using-lda-and-bertopic.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/NLP Topic Modeling of Customer Complaints – A Comparative Approach Using LDA and BERTopic.md</guid><pubDate>Wed, 07 May 2025 12:28:23 GMT</pubDate><enclosure url="lib\media\pyldavis-visualization-v2.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\pyldavis-visualization-v2.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Performance improvement of LLMs for NER using Ensemble Learning techniques]]></title><description><![CDATA[ 
 <br><br>The thesis titled "Performance Improvement of Large Language Models for Named Entity Recognition using Ensemble Learning Techniques" focuses on leveraging ensemble learning methods, specifically Bagging and Boosting, to enhance the performance of Named Entity Recognition (NER) tasks in Large Language Models (LLMs). NER plays a crucial role in Natural Language Processing (NLP) tasks by identifying and categorizing entities such as names, locations, dates, and more in unstructured text. However, NER often faces challenges such as false positives (FP), where non-entities are incorrectly identified as entities, and false negatives (FN), where actual entities are missed. These errors can significantly affect the accuracy and reliability of the models.<br>The main goal of this research was to explore whether ensemble learning methods like Bagging (which combines multiple models to reduce overfitting and error) and Boosting (which sequentially corrects misclassifications of previous models) could reduce these common errors and improve NER performance. The experiment utilized the DocRED dataset, which is a human-labeled dataset commonly used in the evaluation of NER tasks, providing ground truth for comparison. The study also aimed to evaluate whether these methods could be practical in real-world applications by considering computational cost, processing time, and scalability.<br><br>
<br>
Data Processing &amp; Evaluation Strategy

<br>Fuzzy Matching was applied to account for minor token misalignments, allowing for variations in entity positions while comparing predictions to ground truth.
<br>The evaluation was based on key metrics: Precision, Recall, and F1-score, which are critical in determining the accuracy of NER tasks, focusing on the ability to reduce FP and FN.
<br>A confusion matrix was used to calculate performance, offering a transparent method for measuring the effectiveness of NER extraction.


<br>
Model Development &amp; Performance Analysis

<br>The Bagging and Boosting approaches were compared to standard NER models. Bagging showed a reduction in mismatched and redundant entities (FP), while Boosting was more effective at reducing missed entities (FN).
<br>Performance analysis was conducted by tracking FN, FP, and F1-scores across various iterations, with Bagging showing a slight performance boost (+0.7% F1-score) and Boosting causing a minor decline in F1-score despite reductions in FN.


<br>
Feasibility &amp; Practicality Assessment

<br>Both ensemble methods were found to improve performance in specific error categories, but the cost-effectiveness of these methods was questioned. The increase in computational time and resources, especially in Boosting, was deemed impractical for general application, despite the improvements in reducing FN (Boosting) and FP (Bagging).
<br>Specialized Use Cases where targeted error reduction (such as lowering FN or FP in legal or medical texts) could benefit from these techniques were identified as more appropriate scenarios for applying these methods


<br><br>
<br>Practical Insights on Ensemble Learning for NER: Demonstrated how Bagging and Boosting methods could reduce errors in NER tasks, with a clear identification of trade-offs between error types and computational cost.
<br>Key Learning on Evaluation Metrics: Gained a deep understanding of the importance of precision, recall, and F1-score in evaluating NER models, especially in contexts where errors in specific entity types (e.g., missed or mismatched entities) are more critical.
<br>Performance vs. Cost: While the methods showed minor improvements in NER performance, the resource cost (increased processing time) for only a small improvement in accuracy made them impractical for general use, highlighting the need for cost-benefit analysis when deploying ensemble models.
<br>Impact of Dataset Quality: Learned how dataset inconsistencies (such as title variations or ambiguous entity boundaries) can significantly affect model performance, particularly with ensemble techniques. This insight stresses the importance of data cleaning and standardization for NER tasks.
<br>Exploration of NER in Specialized Domains: While general performance improvements were limited, the research pointed to the potential of these methods for specialized applications where targeting specific error categories could lead to significant real-world benefits (e.g., reducing FN in legal documents or reducing FP in medical records).
<br>Future Directions: The thesis also paved the way for further research into context-specific optimization of ensemble methods for NER and other language tasks, proposing the integration of fine-tuning and hybrid models for specialized error handling. This opens new avenues for improving LLM performance in high-stakes applications.
<br><br><br>Github Repository: <a rel="noopener nofollow" class="external-link" href="https://github.com/sanax-997/Performance-improvement-of-LLMs-for-Named-Entity-Recognition-using-Ensemble-Learning-techniques.git" target="_blank">https://github.com/sanax-997/Performance-improvement-of-LLMs-for-Named-Entity-Recognition-using-Ensemble-Learning-techniques.git</a>]]></description><link>01-entries\1.2-education\iu-entries\performance-improvement-of-llms-for-ner-using-ensemble-learning-techniques.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Performance improvement of LLMs for NER using Ensemble Learning techniques.md</guid><pubDate>Wed, 07 May 2025 12:28:28 GMT</pubDate></item><item><title><![CDATA[Principles of Agility used in Modern Software Development]]></title><description><![CDATA[ 
 <br>]]></description><link>01-entries\1.2-education\iu-entries\principles-of-agility-used-in-modern-software-development.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Principles of Agility used in Modern Software Development.md</guid><pubDate>Wed, 07 May 2025 12:15:37 GMT</pubDate></item><item><title><![CDATA[Sentiment Analysis of Customer Reviews – NLP Pipeline Using Pre-Trained BERT Models]]></title><description><![CDATA[ 
 <br><br>This project applies Natural Language Processing (NLP) techniques to analyze customer reviews and classify sentiments using a pre-trained BERT model. Designed as part of an Artificial Intelligence course, the project demonstrates a full NLP pipeline—from data cleaning to prediction, visualization, and evaluation. The Women’s Clothing E-Commerce Reviews dataset from Kaggle served as the foundation, and the implementation leveraged tools like Pandas, Scikit-learn, Transformers, and Matplotlib to process and analyze the text. The final product is a modular, accurate sentiment analysis tool capable of turning unstructured review data into valuable business insights.<br><br>1. Conception Phase<br>
<br>Selected the “Womens Clothing E-Commerce Reviews” dataset from Kaggle for its blend of review text and numerical star ratings.
<br>Researched and defined the NLP pipeline, choosing Python's Transformers (Hugging Face) and scikit-learn for modeling and evaluation.
<br>Chose the bert-base-uncased-finetuned-review-sentiment-analysis model for classification, based on its fine-tuning for customer reviews.
<br>2. Development Phase<br>
<br>Cleaned dataset using Pandas and filtered out entries with empty review text.
<br>Tokenized and preprocessed text data using AutoTokenizer, tailored to the BERT model architecture.
<br>Built a sentiment classification pipeline using Hugging Face’s pipeline() interface to predict review sentiment.
<br>Categorized both predictions and true labels into positive, neutral, and negative classes based on review star ratings.
<br>Visualized the results using Matplotlib and assessed model performance with accuracy, precision, recall, and F1-score.
<br>3. Finalization Phase<br>
<br>Completed a functional, user-ready Python script capable of full-cycle sentiment analysis.
<br>Evaluated performance with a strong overall accuracy of 87%, with high precision and recall for positive sentiments.
<br>Identified opportunities for enhancement in class imbalance (underrepresented negative reviews) and proposed data augmentation and feedback loops for model retraining.
<br><br>
<br>Applied pre-trained transformer models (BERT) to real-world textual data for sentiment analysis.
<br>Integrated full NLP pipeline: text preprocessing, classification, visualization, and evaluation.
<br>Achieved high performance (F1-score of 0.95 for positive class), while acknowledging limitations in negative sentiment classification (F1-score: 0.55).
<br>Demonstrated data storytelling through visual outputs and interpretability of model performance.
<br>Proposed feedback-based optimization strategies for real-world improvements in underperforming classes.
<br>Delivered a clean, reproducible, and well-documented Python-based tool ready for integration or expansion.
<br><br>The development of the sentiment analysis application began in the conception phase, where the primary objective was defined: to classify customer sentiment based on product reviews using Natural Language Processing (NLP). The project was built upon the “Women’s Clothing E-Commerce Reviews” dataset from Kaggle, chosen for its rich combination of unstructured text and associated user ratings. Early exploration of the dataset using Pandas revealed missing entries and inconsistent formatting in the text data. A filtering step was applied to remove reviews with no textual content, forming the foundation for the subsequent NLP pipeline.<br><img alt="Ablauf Diagram.jpg" src="lib\media\ablauf-diagram.jpg"><br>The next stage involved tokenization and preprocessing of the review text. To achieve compatibility with the planned transformer model, a tokenizer from the Hugging Face Transformers library was used—specifically tailored to the BERT model selected for this task. Preprocessing steps such as lowercasing, removal of special characters, and handling of negations were handled internally during tokenization. This allowed the use of raw text input while preserving the model’s performance, as BERT-based transformers are trained on unprocessed, natural text to retain contextual understanding.<br>Once the data was tokenized, the project entered the modeling phase, where the pre-trained BERT model (bert-base-uncased-finetuned-review-sentiment-analysis) was initialized. The model was loaded via the Hugging Face pipeline() method, streamlining inference by accepting batches of review text and returning sentiment predictions. These predictions were generated in real time, with each review labeled as “positive,” “neutral,” or “negative” based on the model’s internal classification probabilities.<br>Following prediction, a post-processing step was implemented to categorize the predicted labels and align them with the original user-provided ratings. Star ratings of 4 and 5 were mapped to “positive,” 3 to “neutral,” and 1 or 2 to “negative.” This categorization made it possible to directly compare the model’s output with actual user sentiment, enabling the calculation of performance metrics.<br>To gain a visual overview of the model’s output, a bar plot of sentiment distribution was generated using Matplotlib. Sentiment categories were color-coded for clarity (green for positive, gray for neutral, red for negative). This visualization not only highlighted the class imbalance in the dataset—where positive reviews dominated—but also served as an intuitive summary of the model’s predictions.<br><img alt="sentiment_distribution.png" src="lib\media\sentiment_distribution.png"><br>The final component of the development pipeline was the evaluation phase, where the model’s performance was quantitatively assessed using scikit-learn. Metrics such as accuracy, precision, recall, and F1-score were calculated to measure classification quality. The model achieved an overall accuracy of 87%, with outstanding performance in the positive sentiment category (F1-score of 0.95). However, lower scores for neutral and negative classes indicated underrepresentation in the dataset and highlighted areas for improvement.<br><img alt="Evaluation Metrics.jpg" src="lib\media\evaluation-metrics.jpg"><br>In the finalization phase, these insights were further contextualized. The model’s strength in detecting positive sentiment was affirmed, but its struggle with “negative” and “neutral” classifications prompted a proposal for data augmentation and feedback-based retraining. Suggested improvements included balancing the dataset, incorporating synthetic examples, and using iterative training strategies based on misclassification patterns.<br>Overall, the project demonstrated a complete NLP pipeline built on modern tools and frameworks, delivering a functional sentiment analysis system with high interpretability and extensibility. It successfully combined pre-trained deep learning models with real-world data and offered practical takeaways for product review mining and customer experience analysis.<br><br>Github Repository: <a rel="noopener nofollow" class="external-link" href="https://github.com/sanax-997/nlp_customer_complaints_analysis.git" target="_blank">https://github.com/sanax-997/nlp_customer_complaints_analysis.git</a>]]></description><link>01-entries\1.2-education\iu-entries\sentiment-analysis-of-customer-reviews-–-nlp-pipeline-using-pre-trained-bert-models.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Sentiment Analysis of Customer Reviews – NLP Pipeline Using Pre-Trained BERT Models.md</guid><pubDate>Wed, 07 May 2025 12:28:17 GMT</pubDate><enclosure url="lib\media\ablauf-diagram.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib\media\ablauf-diagram.jpg&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Testing statistical hypotheses using non-parametric tests]]></title><description><![CDATA[ 
 <br>]]></description><link>01-entries\1.2-education\iu-entries\testing-statistical-hypotheses-using-non-parametric-tests.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/Testing statistical hypotheses using non-parametric tests.md</guid><pubDate>Wed, 07 May 2025 12:15:26 GMT</pubDate></item><item><title><![CDATA[What is the Historical Development of CAR-2-X Technology]]></title><description><![CDATA[ 
 <br>]]></description><link>01-entries\1.2-education\iu-entries\what-is-the-historical-development-of-car-2-x-technology.html</link><guid isPermaLink="false">01 - Entries/1.2 - Education/IU Entries/What is the Historical Development of CAR-2-X Technology.md</guid><pubDate>Wed, 07 May 2025 12:16:13 GMT</pubDate></item><item><title><![CDATA[Professional Title - Ingenieur (Ing.)]]></title><description><![CDATA[ 
 <br>The professional title "Ingenieur" (abbreviated as "Ing.") in Austria is a state-recognized qualification awarded by the Austrian Federal Ministry of Education, Science and Research. It represents a high level of technical and practical expertise in engineering and applied sciences. Although it is not an academic degree, it is formally classified at Level 6 of the Austrian National Qualifications Framework, which corresponds to the level of a bachelor’s degree in terms of competence and skills.<br>The title is granted to individuals who have completed a Higher Technical Institute (HTL), which is a five-year technical secondary school program, and who have subsequently acquired at least three years of relevant professional experience in their field. In addition to fulfilling these requirements, candidates must successfully undergo a formal assessment process, which evaluates their applied knowledge and professional competence. The "Ing." title is highly regarded in Austria and reflects a strong tradition of practice-oriented technical education and vocational achievement.]]></description><link>01-entries\1.3-certifications-or-professional-development\professional-title-ingenieur-(ing.).html</link><guid isPermaLink="false">01 - Entries/1.3 - Certifications or Professional Development/Professional Title - Ingenieur (Ing.).md</guid><pubDate>Thu, 01 May 2025 11:47:26 GMT</pubDate></item></channel></rss>